{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48aad21d",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial: babI6 Training and Preprocessing in Python\n",
    "\n",
    "In this tutorial, we will try to implement question asking and answering for babI6 tasks. babI6 tasks are tasks where we supply a text that describes movement of people in different locations and ask questions about the locations of said people while they are moving around. More on the babI tasks can be in this [paper](https://arxiv.org/abs/1502.05698) and this [repository](https://github.com/facebookarchive/bAbI-tasks?tab=readme-ov-file) by Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf759bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ouissal.moumou/actual_discocirc/notebooks/discocirc-experiments/lambeq/experimentsenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from lambeq.experimental.discocirc import DisCoCircReader\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20186c0",
   "metadata": {},
   "source": [
    "Before we delve into the code, we first highlight two new features of the new parser that will be used in this tutorial: the sandwich functor and foliated frames. \n",
    "\n",
    "In the previous versions of the parser, the semantic functor, while providing quantum implementations for boxes, wires and states, did not specify the quantum implementation of frames. The sandwich functor addresses this issue by introducing a novel construction that breaks down a frame into a sequence of boxes with the frame's contents. Now that we have these different frames, we can decide whether we want every layer in these frames to be assigned their operator or have the same operator for all the layers (different parameters assigned to the layers as opposed to having all the layers having the same parameter). For more detail on this, we recommend reading the paper explaining  the theory behind the new parser [here](in_the_making)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d138",
   "metadata": {},
   "source": [
    "## 1. Setting Up Configuration Variables\n",
    "\n",
    "This cell defines paths and key configuration variables:\n",
    "- `FILEPATH` specifies paths to the file containing the babI6 data. \n",
    "- `TEXT_LENGTH` specifies the maximum number of sentences in a context for the experiment.\n",
    "- `MAX_WIDTH` specifies the maximum number of wires in a circuit for the experiment.\n",
    "- `SANDWICH` is a flag for using the sandwich functor: True to apply the sandwich functor on the circuits, False to apply the usual semantic functor.\n",
    "- `FFL` is a flag for activating the foliated frames. True to set different parameters for the different layers of frames, False to set the same parameter for all the layers. It is to note that it only makes sense to have this flag if the sandwich functor is activated.\n",
    "- `TRAINING_SAMPLE_SIZE`, `VALIDATION_SAMPLE_SIZE`, and `TEST_SAMPLE_SIZE` specify the size of the data for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28993a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store all the variables needed for the rest of the code: file paths, configurations, parameters...\n",
    "\n",
    "# The path of the file where the initial babI6 data is stored\n",
    "FILEPATH = '../data/qa6_train_10k.txt'\n",
    "\n",
    "# Maximum length of the context\n",
    "TEXT_LENGTH = 4\n",
    "\n",
    "# Maximum Number of wires in a circuit\n",
    "MAX_WIDTH = 9\n",
    "\n",
    "# SANDWICH functor flag\n",
    "SANDWICH = True\n",
    "\n",
    "# Updating the FFL parameter\n",
    "FFL = True\n",
    "\n",
    "# Sizes of training, validation, and test datasets\n",
    "TRAINING_SAMPLE_SIZE = 120\n",
    "VALIDATION_SAMPLE_SIZE = 30\n",
    "TEST_SAMPLE_SIZE = 30\n",
    "\n",
    "# Paths of resulting files for the datasets\n",
    "TRAINING_DATASET_FILEPATH = 'tutorial_training_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "VALIDATION_DATASET_FILEPATH = 'tutorial_validation_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "TEST_DATASET_FILEPATH = 'tutorial_test_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2f18",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Function\n",
    "\n",
    "The next step is to write a function `task_file_reader`, which processes the babI6 dataset and returns a list of texts, a list of questions on these texts, a list of answers to these questions, and a list of the lengths of the texts. This function reads and cleans lines from the `FILEPATH`, splits lines into stories, and extracts text sentences, questions, and answers.\n",
    "\n",
    "After extracting the texts, they are filtered to only keep the ones whose number of sentences is less than or equal to `TEXT_LENGTH`, which we set in the previous cell to determine the maximum number of sentences that we want in a text for optimization purposes. We want to make sure that we do not get huge circuits later on when we convert the texts into circuits, which might slow down the experiment. \n",
    "\n",
    "After this filtering, the last step is to convert the list of texts from a list of arrays of sentences, to a list of sentences. In other words, we concatenate the sentences in each text (which is an array) to obtain a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e8f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the texts, questions, expected answers, and text_length from the TXT file\n",
    "def task_file_reader(path : str | Path) -> Tuple[List[List[str]],\n",
    "                                                       List[str],\n",
    "                                                       List[str],\n",
    "                                                       List[str]]:\n",
    "    \"\"\"\n",
    "    reads the .txt file at `path`\n",
    "    returns 3 lists of equal length\n",
    "    - text sentences, questions, answers and text length\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # split the lines into stories\n",
    "    # record the first line location of new stories\n",
    "    story_splits = [i for i, line in enumerate(lines) if line[0:2] == '1 ']\n",
    "    # have no more need for line indices - delete these\n",
    "    lines = [' '.join(line.split(' ')[1:]) for line in lines]\n",
    "    # also delete . and \\n\n",
    "    lines = [line.replace('.', '').replace('\\n','') for line in lines]\n",
    "    stories = [lines[i:j] for i, j in zip(story_splits, story_splits[1:]+[None])]\n",
    "\n",
    "    # create text and QnA pairs\n",
    "    texts = []\n",
    "    qnas = []\n",
    "    text_length = []\n",
    "    for story in stories:\n",
    "        # record the lines in the story corresponding to questions\n",
    "        question_splits = [i for i, line in enumerate(story) if '?' in line]\n",
    "        for index in question_splits:\n",
    "            # record the text corresponding to each question\n",
    "            ctx = [line.lower() for line in story[:index] if '?' not in line]\n",
    "            texts.append(ctx)\n",
    "            text_length.append(len(ctx))\n",
    "            # record the question\n",
    "            qnas.append(story[index])\n",
    "\n",
    "    # split qna into questions and answers\n",
    "    questions = [qna.split('\\t')[0].lower().rstrip()[:-1] + \" ?\" for qna in qnas]\n",
    "    answers = [qna.split('\\t')[1].lower() for qna in qnas]\n",
    "\n",
    "    # Filtering the data   \n",
    "    filtered_data = [\n",
    "    (text, question, answer, text_length)\n",
    "    for text, question, answer, text_length in zip(texts, questions, answers, text_length)\n",
    "    if len(text) <= TEXT_LENGTH\n",
    "    ]\n",
    "\n",
    "    # Applying the filter\n",
    "    texts, questions, answers, text_length = map(list, zip(*filtered_data))\n",
    "\n",
    "    # Converting the texts from arrays of sentences to strings\n",
    "    processed_texts_list = []\n",
    "    for text in texts:\n",
    "        processed_text = \"\"\n",
    "        for sentence in text:\n",
    "            processed_text += sentence + \". \"\n",
    "                \n",
    "        processed_texts_list.append(processed_text)\n",
    "    \n",
    "\n",
    "    return processed_texts_list, questions, answers, text_length\n",
    "\n",
    "\n",
    "texts, questions, answers, text_lengths = task_file_reader(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67428372",
   "metadata": {},
   "source": [
    "## 3. Converting The Texts into Circuits\n",
    "\n",
    "Now that we have our texts and the rest of data ready and pre-processed, we move on to the crucial step of converting them into circuits. We first start by initializing the reader, then we use it with the `SANDWICH` flag indicating whether to use the SANDWICH functor or not, as well as the `FFL` flag which indicates whether to assign different parameters to the different layers of the frame, or the same parameters.\n",
    "\n",
    "Moreover, we store the data in a dictionary where each entry includes the text, the corresponding generated DisCoCirc circuit, the question, the answer, and the text_length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbca676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making the circuits from the texts and storing them in the dictionary\n",
    "reader = DisCoCircReader()\n",
    "datadict = {}\n",
    "for i, (text, quest, ans, text_length) in enumerate(zip(texts, questions, answers, text_lengths)):\n",
    "    datadict.update({i:{'text':text, 'dsc_diag': reader.text2circuit(text, sandwich=SANDWICH, foliated_frame_labels = FFL),  'question':quest, 'answer':ans, 'text_length': text_length}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150eddc",
   "metadata": {},
   "source": [
    "## 4. Converting The Circuits from DisCoCirc Circuits to Quantum Circuits\n",
    "While we have the circuits corresponding to the texts ready, they are still DisCoCirc circuits, not quantum circuits. Therefore, we need to convert the DisCocirc circuits into quantum circuits by applying an ansatz. In this case, we chose to apply the Sim4Ansatz with 3 layers, and one qubit for each noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73465ead",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m ansatz \u001b[38;5;241m=\u001b[39m Sim4Ansatz({N:\u001b[38;5;241m1\u001b[39m}, n_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m datadict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 8\u001b[0m     datadict[i]\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_circuit_sim4_13\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mansatz\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatadict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdsc_diag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m})\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/ansatz/circuit.py:114\u001b[0m, in \u001b[0;36mCircuitAnsatz.__call__\u001b[0;34m(self, diagram)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, diagram: Diagram) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Circuit:\n\u001b[1;32m    113\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a lambeq diagram into a lambeq circuit.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiagram\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:1966\u001b[0m, in \u001b[0;36mFunctor.__call__\u001b[0;34m(self, entity)\u001b[0m\n\u001b[1;32m   1964\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(entity)\n\u001b[1;32m   1965\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mar_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:1991\u001b[0m, in \u001b[0;36mFunctor.ar_with_cache\u001b[0;34m(self, ar)\u001b[0m\n\u001b[1;32m   1988\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ar\u001b[38;5;241m.\u001b[39mis_id:\n\u001b[0;32m-> 1991\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_functor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1993\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_category\u001b[38;5;241m.\u001b[39mDiagram\u001b[38;5;241m.\u001b[39mid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mob_with_cache(ar\u001b[38;5;241m.\u001b[39mdom))\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:1244\u001b[0m, in \u001b[0;36mDiagram.apply_functor\u001b[0;34m(self, functor)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m   1243\u001b[0m     left, box, right \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39munpack()\n\u001b[0;32m-> 1244\u001b[0m     diagram \u001b[38;5;241m>>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_diagram\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m                 \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfunctor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m diagram\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:923\u001b[0m, in \u001b[0;36mDiagram.__matmul__\u001b[0;34m(self, rhs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__matmul__\u001b[39m(\u001b[38;5;28mself\u001b[39m, rhs: Diagrammable \u001b[38;5;241m|\u001b[39m Ty) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:917\u001b[0m, in \u001b[0;36mDiagram.tensor\u001b[0;34m(self, *diagrams)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m diagram \u001b[38;5;129;01min\u001b[39;00m diags:\n\u001b[1;32m    916\u001b[0m     right \u001b[38;5;241m=\u001b[39m right[\u001b[38;5;28mlen\u001b[39m(diagram\u001b[38;5;241m.\u001b[39mdom):]\n\u001b[0;32m--> 917\u001b[0m     layers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mextend(left, right) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m diagram\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m    918\u001b[0m     left \u001b[38;5;241m@\u001b[39m\u001b[38;5;241m=\u001b[39m diagram\u001b[38;5;241m.\u001b[39mcod\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(dom\u001b[38;5;241m=\u001b[39mdom, cod\u001b[38;5;241m=\u001b[39mleft, layers\u001b[38;5;241m=\u001b[39mlayers)\n",
      "File \u001b[0;32m~/actual_discocirc/notebooks/discocirc-experiments/lambeq/lambeq/backend/grammar.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m diagram \u001b[38;5;129;01min\u001b[39;00m diags:\n\u001b[1;32m    916\u001b[0m     right \u001b[38;5;241m=\u001b[39m right[\u001b[38;5;28mlen\u001b[39m(diagram\u001b[38;5;241m.\u001b[39mdom):]\n\u001b[0;32m--> 917\u001b[0m     layers \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39mextend(left, right) \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m diagram\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[1;32m    918\u001b[0m     left \u001b[38;5;241m@\u001b[39m\u001b[38;5;241m=\u001b[39m diagram\u001b[38;5;241m.\u001b[39mcod\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(dom\u001b[38;5;241m=\u001b[39mdom, cod\u001b[38;5;241m=\u001b[39mleft, layers\u001b[38;5;241m=\u001b[39mlayers)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lambeq import Sim4Ansatz\n",
    "from lambeq import AtomicType\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "\n",
    "for i in datadict.keys():\n",
    "    datadict[i].update({'text_circuit_sim4_13': ansatz(datadict[i]['dsc_diag'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdeb9a1",
   "metadata": {},
   "source": [
    "## 5. Question Asking Circuits and Further Processing of the Circuits\n",
    "The main spirit of this tutorial is having questions also be circuits that one sequetially composes with the circuits representing texts in order to see the similarity between the texts and guess the answer to the question by performing postselections. More details on question asking can be found [here](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "Now that we already have the circuits representing the texts, we need to make the circuits representing the questions. Remember, in our experiment, we need to have a pair of circuits, one for the affirmative case, and the other for the negative case. However, when adding the box corresponding to the question, we have to make sure that the wires of the question box correspond to the nouns from the text that are asked about.  \n",
    "\n",
    "Below, the function `return_noun_list` returns all the nouns in a text and the function `return_q_nouns` return all the nouns in a question. One can notice that in the latter, we take the second and fifth noun as the subject and object of the question respectively. This works because of the simple case of the babI6 experiments, all the questions are of the format \"Is the subject in the location?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15baa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Ty\n",
    "\n",
    "def return_noun_list(text):\n",
    "    noun_list = []\n",
    "    for b in text.boxes:\n",
    "        if b.dom == Ty() and b.cod == N:\n",
    "            noun_list.append(b.name)\n",
    "            \n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_q_nouns(question):\n",
    "    question_words = question.split(' ')\n",
    "    q_nouns = [question_words[1], question_words[4].strip('?')]\n",
    "    return q_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae48f77",
   "metadata": {},
   "source": [
    "We proceed to add the lists obtained from the functions above to the dictionary to be used later once we want to add the question asking boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datadict.keys():\n",
    "    datadict[i].update({'noun_list_text': return_noun_list(datadict[i]['dsc_diag'])})\n",
    "    datadict[i].update({'noun_list_question': return_q_nouns(datadict[i]['question'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a2617",
   "metadata": {},
   "source": [
    "We needed to extract the list of nouns in the texts as well as the list of nouns in their corresponding questions to remove the entries where the question nouns include nouns that are not in the text (questions that ask about subjects or locations not present in the text. We do this for simplification purposes). The following cell checks every entry from the `datadict` dictionary and adds the ids of \"bad\" entries (entries where the question contains nouns not present in the text) to the `discarded_ids` list that will be used later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce texts where question nouns are not in the text\n",
    "discarded_datalist = []\n",
    "discarded_ids = []\n",
    "for i in datadict.keys():\n",
    "    text_nouns = datadict[i]['noun_list_text']\n",
    "    q_nouns = datadict[i]['noun_list_question']\n",
    "    for noun in q_nouns:\n",
    "        if noun not in text_nouns:\n",
    "            discarded_datalist.append(datadict[i])\n",
    "            discarded_ids.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deda19b",
   "metadata": {},
   "source": [
    "Now that we have a list of ids of entries that should be discarded because they contain nouns in the questions that are not present in the reduced contexts corresponding to them, we simply remove the said entries from the `datadict` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b15b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_datadict = {}\n",
    "for i in datadict.keys():\n",
    "    if i not in discarded_ids:\n",
    "        if datadict[i]['text_length'] < TEXT_LENGTH:\n",
    "            reduced_datadict.update({i:datadict[i]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb6e7e",
   "metadata": {},
   "source": [
    "Moreover, remember that for efficiency purposes, we also wanted to limit the number of wires in every circuit by chekcing that every circuit's codomain (which is the number of open wires of a circuit) is less than or equal to `MAX_WIDTH`. The following filters the entries in the `datadict` dictionary and only keep the entries in which the text circuits have less than or equal to  the maximum number of wires specified in `MAX_WIDTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af318e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducing the size of the dictionary by removing the circuits that have more than a certain number of wires. \n",
    "def right_cod_size(circuit):\n",
    "    if len(circuit.cod) > MAX_WIDTH:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def filter_entries_by_pos_neg_circuit_pair(entries):\n",
    "    filtered_entries = {}\n",
    "    \n",
    "    for key, entry in entries.items():\n",
    "        pos_neg_circuit_pair = entry.get('text_circuit_sim4_13')\n",
    "        if pos_neg_circuit_pair and right_cod_size(pos_neg_circuit_pair):\n",
    "            filtered_entries[key] = entry\n",
    "    \n",
    "    return filtered_entries\n",
    "\n",
    "filtered_cod_datadict = filter_entries_by_pos_neg_circuit_pair(reduced_datadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a8ff9",
   "metadata": {},
   "source": [
    "Now that the text circuits are post-processed for optimization, it is time to make the question asking circuits to later sequentially compose the latter with the former. \n",
    "\n",
    "We first start with constructing two boxes `q1` and `q2` for both the affirmative and negative questions respectively. An affirmative question is just the same question that came with the text (in the pre-processing step). On the other hand, the negative question refers to the question on the negative case of the affirmative questions. For example, if the question related to a text is \"Is Emily in the kitchen?\", the equivalent negative question would be \"Is Emily not in the kitchen?\". For the purposes of this training, all the questions are either in the format of \"Is subject in object?\" or \"Is subject not in object?\". Therefore, we will need two boxes for the questions, a box for the \"is in\" questions, and another for the \"is not in\" questions. The purpose of having two generic boxes is that the ML model will learn later the parameters for these boxes. For more details about question asking in DiscoCir, we recommend [this paper](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "Notice that we also created two question boxes that are equiped with swaps, the purpose of which will become clearer in later parts of the tutorial. \n",
    "\n",
    "We apply the same ansatz applied on the text circuits (Sim4Ansatz with 3 layers and one qubit for each wire). Lastly, we add the postselections by sequentially composing the resulting circuit from applying the ansatz to a parallel composition of two effects (bras). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ea6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Box, Id\n",
    "from lambeq.backend.quantum import  Bra, Discard, qubit, Id, Swap\n",
    "from lambeq import AtomicType, Sim4Ansatz\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "\n",
    "q1 = Box('is_in', N@N, N@N)\n",
    "q2 = Box('is_not_in', N@N, N@N)\n",
    "\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "qcirc1 = ansatz(q1)\n",
    "qcirc2 = ansatz(q2)\n",
    "\n",
    "#add the postselections to the questions\n",
    "qcirc1_final = qcirc1 >> Bra(0) @ Bra(0)\n",
    "qcirc2_final = qcirc2 >> Bra(0) @ Bra(0)\n",
    "\n",
    "is_in_q = qcirc1_final\n",
    "is_not_in_q = qcirc2_final\n",
    "\n",
    "is_in_q_swp = Swap(qubit, qubit) >> qcirc1 >> Bra(0) @ Bra(0)\n",
    "is_not_in_q_swp = Swap(qubit, qubit) >> qcirc2 >> Bra(0) @ Bra(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f120a",
   "metadata": {},
   "source": [
    "## 5. Assembling The Text Circuits with the Question Circuits\n",
    "\n",
    "Now that we have all the ingredients in place (the text and question circuits), it is time to assemble them using sequential composition. However, we need to be careful and only attach the wires in the question boxes to the corresponding wires in the text boxes so that the nouns match. Moreover, we have to discard the wires of the nouns that are not included in the question. In order to do this, we might need to perform some swaps so that the wires that get composed with the question circuit are the corresponding wires from the text circuit.\n",
    "\n",
    "We start by creating a layer composed of either identities (to link with the wires corresponding to the question nouns), or discards (for the rest of the wires). Once we sequentially compose this layer with the text circuit, this leaves us with a circuit whose codomain has two wires corresponding to the question. In order for us to attach the question boxes, we have to make sure that the wires from the question circuits are linked to the right wires from the text circuit. To achieve this, we check the question ids of the wires in the text circuits (to see whether the nouns in the text circuits are in the right order). This helps us decide whether to use the quetion boxes that come with swaps, or the ones without swaps (if the question wires are in the wrong order, we would need a swap to bring them back to the right order for the questions. Remember, we already created question boxes that are also equiped with swaps for this purpose).\n",
    "\n",
    "Notice that, throughout the next cell, we always have two circuits. The circuit names ending in \"pos\" signal the circuits corrsponding to the affirmative questions, while their counterparts ending in \"neg\" signal the ones corresponding to the negative questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing everything together by plugging the question asking part to the text circuits\n",
    "for i in reduced_datadict.keys():\n",
    "    \n",
    "    text_circuit = reduced_datadict[i]['text_circuit_sim4_13']\n",
    "    text_nouns = datadict[i]['noun_list_text']\n",
    "    q_nouns = datadict[i]['noun_list_question']\n",
    "    qid1 = datadict[i]['noun_list_text'].index(q_nouns[0])\n",
    "    qid2 = datadict[i]['noun_list_text'].index(q_nouns[1])\n",
    "\n",
    "    swap_required = False\n",
    "    \n",
    "    if qid1 > qid2: \n",
    "        swap_required = True\n",
    "    elif qid1 < qid2:\n",
    "        swap_required = False\n",
    "    else:\n",
    "        print('noun ids are idential, error')\n",
    "\n",
    "    quest_mid_layer = Id(qubit) if (qid1 == 0 or qid2 == 0) else Discard()\n",
    "    \n",
    " \n",
    "    for k in range(1, len(text_circuit.cod)):\n",
    "        if k == qid1 or k == qid2:\n",
    "            quest_mid_layer = quest_mid_layer @ Id(qubit)\n",
    "        else:\n",
    "            quest_mid_layer = quest_mid_layer @ Discard()\n",
    "\n",
    "    final_circuit = text_circuit >> quest_mid_layer\n",
    "\n",
    "    if swap_required:\n",
    "        final_circuit_pos = final_circuit >> is_in_q_swp\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q_swp\n",
    "    else:\n",
    "        final_circuit_pos = final_circuit >> is_in_q\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q\n",
    "\n",
    "    reduced_datadict[i].update({'pos_neg_circuit_pair': (final_circuit_pos, final_circuit_neg)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd17926",
   "metadata": {},
   "source": [
    "## 6. Preparing The Datasets for Training\n",
    "Now that our circuit pairs are ready, we move on to the final step of training a model.\n",
    "\n",
    "The first step is to prepare the data for training. We start with updating the \"yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8aecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "babi6_datadict = {}\n",
    "for i in reduced_datadict.keys():\n",
    "    # Add the updated dictionary with the transformed 'answer'\n",
    "    babi6_datadict.update({\n",
    "        i: {\n",
    "            'text': reduced_datadict[i]['text'],\n",
    "            'question': reduced_datadict[i]['question'],\n",
    "            'answer': 1 if reduced_datadict[i]['answer'] == 'yes' else 0,  # Transform 'yes' to 1 and 'no' to 0\n",
    "            'quantum_circ_pair_pos_neg': reduced_datadict[i]['pos_neg_circuit_pair'],\n",
    "            'text_length': reduced_datadict[i]['text_length']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545dc09",
   "metadata": {},
   "source": [
    "The next step would be to make three sets: training, validation, and test sets. We try to balance the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcecde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum size is: 88\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add the 'measure' field to each item\n",
    "for key, value in babi6_datadict.items():\n",
    "    temp = -1 if value['answer'] == 0 else 1\n",
    "    value['measure'] = temp * value['text_length']\n",
    "\n",
    "# Group items by absolute value of measure\n",
    "abs_value_groups = defaultdict(list)\n",
    "for key, value in babi6_datadict.items():\n",
    "    abs_value = abs(value['measure'])\n",
    "    abs_value_groups[abs_value].append((key, value))\n",
    "\n",
    "# Balance signs within each group and ensure diverse sizes\n",
    "new_balanced_dict = {}\n",
    "for abs_value, items in abs_value_groups.items():\n",
    "    # Separate positive and negative items\n",
    "    positive_items = [(k, v) for k, v in items if v['measure'] > 0]\n",
    "    negative_items = [(k, v) for k, v in items if v['measure'] < 0]\n",
    "    \n",
    "    # Determine the maximum balanced size for this group\n",
    "    min_size = min(len(positive_items), len(negative_items))\n",
    "    print(\"the minimum size is: \" + str(min_size))\n",
    "    \n",
    "    # Randomly sample from each group to balance\n",
    "    balanced_positive = random.sample(positive_items, min_size)\n",
    "    balanced_negative = random.sample(negative_items, min_size)\n",
    "    \n",
    "    # Add to the balanced dictionary\n",
    "    for k, v in balanced_positive + balanced_negative:\n",
    "        new_balanced_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert dictionary into list of keys and values\n",
    "keys = list(new_balanced_dict.keys())\n",
    "values = list(new_balanced_dict.values())\n",
    "\n",
    "# Split into training and temporary (validation + testing)\n",
    "train_keys, temp_keys, train_values, temp_values = train_test_split(\n",
    "    keys, values, test_size=0.4, random_state=42  # 60% training, 40% temp\n",
    ")\n",
    "\n",
    "# =Split the temporary set into validation and testing\n",
    "val_keys, test_keys, val_values, test_values = train_test_split(\n",
    "    temp_keys, temp_values, test_size=0.5, random_state=42  # 20% validation, 20% testing\n",
    ")\n",
    "\n",
    "# Reconstruct dictionaries for training, validation, and testing\n",
    "training_dict_babi6 = {k: v for k, v in zip(train_keys, train_values)}\n",
    "validation_dict_babi6 = {k: v for k, v in zip(val_keys, val_values)}\n",
    "test_dict_babi6 = {k: v for k, v in zip(test_keys, test_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd61567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "yes_count = 0\n",
    "no_count = 0\n",
    "for i in training_dict_babi6:\n",
    "    if training_dict_babi6[i]['answer'] == 0:\n",
    "        no_count += 1\n",
    "    else:\n",
    "        yes_count += 1\n",
    "\n",
    "print(yes_count)\n",
    "print(no_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af769549",
   "metadata": {},
   "source": [
    "Now, the final step is to store all of this data in separate files for training, validation, and testing, to be used in part II of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f1a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(training_dict_babi6, file)\n",
    "with open(VALIDATION_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(validation_dict_babi6, file)\n",
    "with open(TEST_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(test_dict_babi6, file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experimentsenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
