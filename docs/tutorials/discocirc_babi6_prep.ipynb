{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48aad21d",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial: bAbI6 Training and Preprocessing in Python\n",
    "\n",
    "In this tutorial, we will try to implement question answering for bAbI6 tasks using the new {py:mod}`~lambeq.experimental.discocirc`. In bAbI6 tasks, a text describes people moving between locations, and the goal is to answer questions about where they are. More on the bAbI tasks can be found in this [paper](https://arxiv.org/abs/1502.05698) and this [repository](https://github.com/facebookarchive/bAbI-tasks?tab=readme-ov-file) by Facebook (now Meta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from lambeq.experimental.discocirc import DisCoCircReader\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20186c0",
   "metadata": {},
   "source": [
    "Before we delve into the code, we first highlight two new features of the new {term}`parser <parser>` that will be used in this tutorial: the {term}`sandwich functor <sandwich functor>` and foliated {term}`frames <frame>`. \n",
    "\n",
    "In previous versions of the {term}`parser <parser>`, the semantic {term}`functor <functor>`, while providing quantum implementations for boxes, wires and states, did not specify the quantum implementation of {term}`frames <frame>`. The {term}`sandwich functor <sandwich functor>` tackles this by breaking down a frame into a sequence of boxes with the frame's content {cite:p}`laakkonen_2024`. Now that we have these different boxes, we can decide how to assign operators to them. We can either give each box its own operator, with different parameters for each box. Or, we can use the same operator for all boxes, meaning all boxes share the same parameters. {cite:p}`krawchuk_2025`. For more details on this, we recommend {cite:p}`krawchuk_2025`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d138",
   "metadata": {},
   "source": [
    "## 1. Setting Up Configuration Variables\n",
    "\n",
    "This cell defines paths and key configuration variables:\n",
    "- `FILEPATH` specifies paths to the file containing the bAbI6 data. \n",
    "- `TEXT_LENGTH` specifies the maximum number of sentences in a text for the experiment.\n",
    "- `MAX_WIDTH` specifies the maximum number of wires in a circuit for the experiment.\n",
    "- `SANDWICH` is a flag for using the {term}`sandwich functor <sandwich functor>`: True to apply the sandwich functor on the circuits, False to apply the usual semantic {term}`functor <functor>`.\n",
    "- `FFL` is a flag for activating the foliated {term}`frames <frame>`. True to set different parameters for the different layers (boxes) of frames. False to set the same parameter for all the layers. It is to note that it only makes sense to have this flag if the sandwich functor is activated.\n",
    "\n",
    "We also define file paths to store the prepared training, validation, and test datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28993a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store all the variables needed for the rest of the code: file paths, configurations, parameters...\n",
    "\n",
    "# The path of the file where the initial babI6 data is stored\n",
    "FILEPATH = '../examples/datasets/babi6_10k.txt'\n",
    "\n",
    "# Maximum length of the text\n",
    "TEXT_LENGTH = 4\n",
    "\n",
    "# Maximum Number of wires in a circuit\n",
    "MAX_WIDTH = 9\n",
    "\n",
    "# SANDWICH functor flag\n",
    "SANDWICH = True\n",
    "\n",
    "# Updating the FFL parameter\n",
    "FFL = False\n",
    "\n",
    "# Paths of resulting files for the datasets for training the model later\n",
    "TRAINING_DATASET_FILEPATH = 'tutorial_training_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "VALIDATION_DATASET_FILEPATH = 'tutorial_validation_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "TEST_DATASET_FILEPATH = 'tutorial_test_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2f18",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Function\n",
    "\n",
    "The next step is to write a function `task_file_reader`, which processes the bAbI6 dataset and returns lists of texts, questions, answers and text lengths. This function returns texts as strings.\n",
    "\n",
    "After extracting the texts, they are filtered to only keep the ones whose number of sentences is less than or equal to `TEXT_LENGTH`, which we set in the previous cell to determine the maximum number of sentences that we want in a text for better efficiency. This is to make sure that we do not get huge circuits later on when we convert the texts into circuits, which might slow down the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e8f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the texts, questions, expected answers, and text_length from the TXT file\n",
    "def task_file_reader(path : str | Path) -> Tuple[List[List[str]],\n",
    "                                                       List[str],\n",
    "                                                       List[str],\n",
    "                                                       List[str]]:\n",
    "    \"\"\"\n",
    "    reads the .txt file at `path`\n",
    "    returns 3 lists of equal length\n",
    "    - text sentences, questions, answers and text length\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # split the lines into stories\n",
    "    # record the first line location of new stories\n",
    "    story_splits = [i for i, line in enumerate(lines) if line[0:2] == '1 ']\n",
    "    # have no more need for line indices - delete these\n",
    "    lines = [' '.join(line.split(' ')[1:]) for line in lines]\n",
    "    # also delete . and \\n\n",
    "    lines = [line.replace('.', '').replace('\\n','') for line in lines]\n",
    "    stories = [lines[i:j] for i, j in zip(story_splits, story_splits[1:]+[None])]\n",
    "\n",
    "    # create text and QnA pairs\n",
    "    texts = []\n",
    "    qnas = []\n",
    "    text_length = []\n",
    "    for story in stories:\n",
    "        # record the lines in the story corresponding to questions\n",
    "        question_splits = [i for i, line in enumerate(story) if '?' in line]\n",
    "        for index in question_splits:\n",
    "            # record the text corresponding to each question\n",
    "            text = [line.lower() for line in story[:index] if '?' not in line]\n",
    "            texts.append(text)\n",
    "            text_length.append(len(text))\n",
    "            # record the question\n",
    "            qnas.append(story[index])\n",
    "\n",
    "    # split qna into questions and answers\n",
    "    questions = [qna.split('\\t')[0].lower().rstrip()[:-1] + \" ?\" for qna in qnas]\n",
    "    answers = [qna.split('\\t')[1].lower() for qna in qnas]\n",
    "\n",
    "    # we convert answers to 0s and 1s to be ready for training later on\n",
    "    answers = [1 if ans == 'yes' else 0 for ans in answers]\n",
    "\n",
    "    # Filtering the data   \n",
    "    filtered_data = [\n",
    "    (text, question, answer, text_length)\n",
    "    for text, question, answer, text_length in zip(texts, questions, answers, text_length)\n",
    "    if len(text) <= TEXT_LENGTH\n",
    "    ]\n",
    "\n",
    "    # Applying the filter\n",
    "    texts, questions, answers, text_length = map(list, zip(*filtered_data))\n",
    "\n",
    "    # Converting the texts from arrays of sentences to strings\n",
    "    processed_texts_list = []\n",
    "    for text in texts:\n",
    "        processed_text = \"\"\n",
    "        for sentence in text:\n",
    "            processed_text += sentence + \". \"\n",
    "                \n",
    "        processed_texts_list.append(processed_text)\n",
    "    \n",
    "\n",
    "    return processed_texts_list, questions, answers, text_length\n",
    "\n",
    "\n",
    "texts, questions, answers, text_lengths = task_file_reader(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67428372",
   "metadata": {},
   "source": [
    "## 3. Converting The Texts into Circuits\n",
    "\n",
    "Now that we have our texts and the rest of data ready and pre-processed, we move on to the crucial step of converting them into circuits. We first start by initializing the {py:class}`~lambeq.experimental.discocirc.DisCoCircReader`, then we use the {py:meth}`~lambeq.experimental.discocirc.DisCoCircReader.text2circuit` with the `sandwich` argument indicating whether to use the {term}`sandwich functor <sandwich functor>` or not, as well as the `foliated_frame_labels` argument which indicates whether to assign different parameters to the different layers of the frame, or the same parameters.\n",
    "\n",
    "Moreover, we store the data in a dictionary where each entry includes the text, the corresponding generated {term}`DisCoCirc` circuit, the question, the answer, and the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbca676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making the circuits from the texts and storing them in the dictionary\n",
    "reader = DisCoCircReader()\n",
    "datadict = {}\n",
    "for i, (text, quest, ans, text_length) in enumerate(zip(texts, questions, answers, text_lengths)):\n",
    "    datadict.update({i:{'text':text, 'dsc_diag': reader.text2circuit(text, sandwich=SANDWICH, foliated_frame_labels = FFL),  'question':quest, 'answer':ans, 'text_length': text_length}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150eddc",
   "metadata": {},
   "source": [
    "## 4. Converting The Circuits from DisCoCirc Circuits to Quantum Circuits\n",
    "While we have the circuits corresponding to the texts ready, they are still {term}`DisCoCirc` circuits, not quantum circuits. Therefore, we need to convert the DisCocirc circuits into {term}`quantum circuits <quantum circuit>` by applying an {term}`ansatz <ansatz (plural: ansätze)>`. In this case, we choose to apply the Sim4Ansatz with 3 layers, and one {term}`qubit <qubit>` for each noun. This choice of anzatz has shown good experiemental results. More information on the motivation behind this choice can be found [here](https://arxiv.org/pdf/2409.08777)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73465ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import Sim4Ansatz\n",
    "from lambeq import AtomicType\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "\n",
    "for i in datadict.keys():\n",
    "    datadict[i].update({'text_circuit_sim4_13': ansatz(datadict[i]['dsc_diag'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdeb9a1",
   "metadata": {},
   "source": [
    "## 5. Assertion Circuits and Further Processing of the Circuits\n",
    "The main spirit of this tutorial is having assertion circuits sequetially composed with the text circuits to see the similarity between the texts and the assertions. More details on assertions and implementation of questions in general can be found [here](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "Now that we already have the circuits representing the texts, we need to make the circuits representing the assertions. Remember, in our experiment, we need to have a pair of circuits, one for the affirmative case, and the other for the negative case. However, when adding the box corresponding to the assertion, we have to make sure that the wires of the assertion box match with the wires representing the nouns from the text.  \n",
    "\n",
    "Below, the function `return_noun_list` returns all the nouns in a text. The function `return_q_nouns` return all the nouns in a question. In the latter, we take the third and sixth word as the person and location in the question respectively. This works because of the simple case of the bAbI6 experiments, all the questions are of the format \"Is the person in the location?\".\n",
    "\n",
    "**Important note**: It is to note that this isn't the standard way to implement the questions/assertions, we went for the simplest approach in this tutorial for the sake of simplicity. More complex approaches to assertions on text can be found [here](https://arxiv.org/pdf/2409.08777). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15baa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Ty\n",
    "\n",
    "def return_noun_list(text):\n",
    "    noun_list = []\n",
    "    for b in text.boxes:\n",
    "        if b.dom == Ty() and b.cod == N:\n",
    "            noun_list.append(b.name)\n",
    "            \n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32d767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_q_nouns(question):\n",
    "    question_words = question.split(' ')\n",
    "    q_nouns = [question_words[1], question_words[4].strip('?')]\n",
    "    return q_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae48f77",
   "metadata": {},
   "source": [
    "We proceed to add the lists obtained from the functions above to the dictionary to be used later when building the assertion boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eeefa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datadict.keys():\n",
    "    datadict[i].update({'noun_list_text': return_noun_list(datadict[i]['dsc_diag'])})\n",
    "    datadict[i].update({'noun_list_question': return_q_nouns(datadict[i]['question'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a2617",
   "metadata": {},
   "source": [
    "We needed to extract the list of nouns in the texts and the list of nouns in their corresponding questions to remove the entries where we ask questions on people or locations not present in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b15b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_datadict = {\n",
    "    i: entry \n",
    "    for i, entry in datadict.items() \n",
    "    if set(entry['noun_list_question']).issubset(entry['noun_list_text'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb6e7e",
   "metadata": {},
   "source": [
    "Moreover, remember that to enhance performance, we also wanted to limit the number of wires in every circuit by checking that every circuit's codomain (which is the number of open wires of a circuit) is less than or equal to `MAX_WIDTH`. The following filters the entries in the `datadict` dictionary and only keeps the entries in which the text circuits have less than or equal to the maximum number of wires specified in `MAX_WIDTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af318e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reducing the size of the dictionary by removing the circuits that have more than a certain number of wires. \n",
    "def right_cod_size(circuit):\n",
    "    if len(circuit.cod) > MAX_WIDTH:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_cod_datadict = {\n",
    "    i: entry \n",
    "    for i, entry in reduced_datadict.items() \n",
    "    if right_cod_size(entry['text_circuit_sim4_13'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a8ff9",
   "metadata": {},
   "source": [
    "Now that the text circuits have been post-processed for optimization, we move on to building the assertion circuits. These will later be sequentially composed with the text circuits.\n",
    "\n",
    "We first start with constructing two boxes `q1` and `q2` for both the affirmative and negative assertions respectively. An affirmative assertion corresponds to an affirmative answer to the question. On the other hand, a negative assertion corresponds to a negative answer to the question. For example, if the question related to a text is \"Is Emily in the kitchen?\", the equivalent negative assertion would be \"Emily is not in the kitchen\". For the purposes of this training, all the questions are either in the format of \"Is person in location?\" or \"Is person not in location\". Therefore, we will need two boxes for the assertions, a box for the \"is in\" assertions, and another for the \"is not in\" assertions. The purpose of having two generic boxes is that the ML model will learn later the parameters for these boxes. For more details on the choice of implementation of assertions in {term}`DisCoCirc`, we recommend [this paper](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "We added two assertion boxes with swaps to match the wires in the text circuit, in case the noun order is flipped.\n",
    "\n",
    "We apply the same {term}`ansatz <ansatz (plural: ansätze)>` applied on the text circuits (Sim4Ansatz with 3 layers and one qubit for each wire). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Box, Id\n",
    "from lambeq.backend.quantum import  Bra, Discard, qubit, Id, Swap\n",
    "from lambeq import AtomicType, Sim4Ansatz\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "\n",
    "q1 = Box('is_in', N@N, N@N)\n",
    "q2 = Box('is_not_in', N@N, N@N)\n",
    "\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "qcirc1 = ansatz(q1)\n",
    "qcirc2 = ansatz(q2)\n",
    "\n",
    "qcirc1_final = qcirc1 >> Bra(0) @ Bra(0)\n",
    "qcirc2_final = qcirc2 >> Bra(0) @ Bra(0)\n",
    "\n",
    "is_in_q = qcirc1_final\n",
    "is_not_in_q = qcirc2_final\n",
    "\n",
    "is_in_q_swp = Swap(qubit, qubit) >> qcirc1 >> Bra(0) @ Bra(0)\n",
    "is_not_in_q_swp = Swap(qubit, qubit) >> qcirc2 >> Bra(0) @ Bra(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f120a",
   "metadata": {},
   "source": [
    "## 5. Assembling The Text Circuits with the Question Circuits\n",
    "\n",
    "Now that we have all the ingredients in place (the text and assertion circuits), it is time to assemble them using sequential composition. We need to connect the assertion wires only to their matching text wires, so the nouns align. This is where the assetion circuits with the swaps will be needed. Moreover, we have to discard the wires of the nouns that are not included in the question.\n",
    "\n",
    "Notice that, throughout the next cell, we always have two circuits. The circuit names ending in \"aff\" signal the circuits corrsponding to the affirmative assertions, while their counterparts ending in \"neg\" signal the ones corresponding to the negative assertions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f50a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing everything together by plugging the assertion circuits to the text circuits\n",
    "for i in reduced_datadict.keys():\n",
    "    \n",
    "    text_circuit = reduced_datadict[i]['text_circuit_sim4_13']\n",
    "    text_nouns = datadict[i]['noun_list_text']\n",
    "    q_nouns = datadict[i]['noun_list_question']\n",
    "    qid1 = datadict[i]['noun_list_text'].index(q_nouns[0])\n",
    "    qid2 = datadict[i]['noun_list_text'].index(q_nouns[1])\n",
    "\n",
    "    swap_required = qid1 > qid2\n",
    "    \n",
    "    if qid1 == qid2:\n",
    "        print('noun ids are idential, removing entry')\n",
    "        del reduced_datadict[i]\n",
    "        continue\n",
    "\n",
    "    quest_mid_layer = Id(qubit) if (qid1 == 0 or qid2 == 0) else Discard()\n",
    "    \n",
    "    for k in range(1, len(text_circuit.cod)):\n",
    "        if k == qid1 or k == qid2:\n",
    "            quest_mid_layer = quest_mid_layer @ Id(qubit)\n",
    "        else:\n",
    "            quest_mid_layer = quest_mid_layer @ Discard()\n",
    "\n",
    "    final_circuit = text_circuit >> quest_mid_layer\n",
    "\n",
    "    if swap_required:\n",
    "        final_circuit_aff = final_circuit >> is_in_q_swp\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q_swp\n",
    "    else:\n",
    "        final_circuit_aff = final_circuit >> is_in_q\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q\n",
    "\n",
    "    reduced_datadict[i].update({'quantum_circ_pair_aff_neg': (final_circuit_aff, final_circuit_neg)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd17926",
   "metadata": {},
   "source": [
    "## 6. Preparing The Datasets for Training\n",
    "\n",
    "With our circuit pairs prepared, we now move on to the final step: training a model. This begins with creating three datasets: training, validation, and test. We ensure each set is balanced in terms of both text length and answers (positive and negative).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcecde44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add the 'measure' field to each item\n",
    "for key, value in reduced_datadict.items():\n",
    "    temp = -1 if value['answer'] == 0 else 1\n",
    "    value['measure'] = temp * value['text_length']\n",
    "\n",
    "# Group items by absolute value of measure\n",
    "abs_value_groups = defaultdict(list)\n",
    "for key, value in reduced_datadict.items():\n",
    "    abs_value = abs(value['measure'])\n",
    "    abs_value_groups[abs_value].append((key, value))\n",
    "\n",
    "# Balance signs within each group and ensure diverse sizes\n",
    "new_balanced_dict = {}\n",
    "max_length = 100\n",
    "for abs_value, items in abs_value_groups.items():\n",
    "        # Separate positive and negative items\n",
    "        positive_items = [(k, v) for k, v in items if v['measure'] > 0]\n",
    "        negative_items = [(k, v) for k, v in items if v['measure'] < 0]\n",
    "        \n",
    "        # Determine the minimum balanced size for this group\n",
    "        max_size = min(len(positive_items), len(negative_items), max_length)\n",
    "        \n",
    "        # Randomly sample from each group to balance\n",
    "        balanced_positive = random.sample(positive_items, max_size)\n",
    "        balanced_negative = random.sample(negative_items, max_size)\n",
    "\n",
    "        # Add to the balanced dictionary\n",
    "        for k, v in balanced_positive + balanced_negative:\n",
    "            new_balanced_dict[k] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1324315d",
   "metadata": {},
   "source": [
    "Lastly, we need to split the data into training, validation and test sets. We ensure that each dataset contains a balanced number of positive and negative answers during the split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b66d62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 62\n",
      "Validation set size: 26\n",
      "Test size set: 16\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Label configurations\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.25\n",
    "test_ratio = 0.25\n",
    "\n",
    "# We group by answer\n",
    "answer_to_keys = defaultdict(list)\n",
    "for key, value in new_balanced_dict.items():\n",
    "    answer = value['answer']\n",
    "    answer_to_keys[answer].append(key)\n",
    "\n",
    "# Initializing the dictionaries for the training, valodation, and test datasets\n",
    "training_dict_bAbI6, validation_dict_bAbI6, test_dict_bAbI6 = {}, {}, {}\n",
    "\n",
    "# For each answer, we split the keys proportionally and add to splits\n",
    "for label, keys in answer_to_keys.items():\n",
    "    random.shuffle(keys)  # shuffle in-place for randomness\n",
    "\n",
    "    total = len(keys)\n",
    "    n_train = int(train_ratio * total)\n",
    "    n_val = int(val_ratio * total)\n",
    "    n_test = total - n_train - n_val  # just to account for any rounding error\n",
    "\n",
    "    train_keys = keys[:n_train]\n",
    "    val_keys = keys[n_train:n_train + n_val]\n",
    "    test_keys = keys[n_train + n_val:]\n",
    "\n",
    "    # Populating the datasets\n",
    "    for k in train_keys:\n",
    "        training_dict_bAbI6[k] = new_balanced_dict[k]\n",
    "    for k in val_keys:\n",
    "        validation_dict_bAbI6[k] = new_balanced_dict[k]\n",
    "    for k in test_keys:\n",
    "        test_dict_bAbI6[k] = new_balanced_dict[k]\n",
    "\n",
    "print(f\"Training set size: {len(training_dict_bAbI6)}\")\n",
    "print(f\"Validation set size: {len(validation_dict_bAbI6)}\")\n",
    "print(f\"Test size set: {len(test_dict_bAbI6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06c935d",
   "metadata": {},
   "source": [
    "The following cell is to check that we have a balanced set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d718b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "yes_count = 0\n",
    "no_count = 0\n",
    "for i in training_dict_bAbI6:\n",
    "    if training_dict_bAbI6[i]['answer'] == 0:\n",
    "        no_count += 1\n",
    "    else:\n",
    "        yes_count += 1\n",
    "\n",
    "print(yes_count)\n",
    "print(no_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378620ac",
   "metadata": {},
   "source": [
    "Now, the final step is to store all of this data in separate files for training, validation, and testing, to be used in part II of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3937b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(training_dict_bAbI6, file)\n",
    "with open(VALIDATION_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(validation_dict_bAbI6, file)\n",
    "with open(TEST_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(test_dict_bAbI6, file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorials_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
