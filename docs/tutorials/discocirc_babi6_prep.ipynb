{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48aad21d",
   "metadata": {},
   "source": [
    "\n",
    "# Tutorial: bAbI6 Training and Preprocessing in Python\n",
    "\n",
    "In this tutorial, we will try to implement question answering for bAbI6 tasks using the new {py:mod}`~lambeq.experimental.discocirc`. bAbI6 tasks are tasks where we supply a text that describes movement of people in different locations and ask questions about the locations of said people while they are moving around. More on the bAbI tasks can be in this [paper](https://arxiv.org/abs/1502.05698) and this [repository](https://github.com/facebookarchive/bAbI-tasks?tab=readme-ov-file) by Facebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf759bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from lambeq.experimental.discocirc import DisCoCircReader\n",
    "import os\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20186c0",
   "metadata": {},
   "source": [
    "Before we delve into the code, we first highlight two new features of the new {term} `parser <parser>` that will be used in this tutorial: the {term}`sandwich functor <sandwich functor>`and foliated {term}`frames <frame>`. \n",
    "\n",
    "In the previous versions of the {term} `parser <parser>`, the semantic {term} `functor <functor>`, while providing quantum implementations for boxes, wires and states, did not specify the quantum implementation of {term}`frames <frame>`. The {term}`sandwich functor <sandwich functor>` tackles this by breaking down a frame into a sequence of boxes with the frame's content {cite:p}`laakkonen_2024`. Now that we have these different frames, we can decide how to assign operators to the layers. We can either give each layer its own operator, with different parameters for each layer. Or, we can use the same operator for all layers, meaning all layers share the same parameters. {cite:p}`krawchuk_2025`. For more detail on this, we recommend reading the paper explaining the theory behind the new parser {cite:p}`krawchuk_2025`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e26d138",
   "metadata": {},
   "source": [
    "## 1. Setting Up Configuration Variables\n",
    "\n",
    "This cell defines paths and key configuration variables:\n",
    "- `FILEPATH` specifies paths to the file containing the bAbI6 data. \n",
    "- `TEXT_LENGTH` specifies the maximum number of sentences in a context for the experiment.\n",
    "- `MAX_WIDTH` specifies the maximum number of wires in a circuit for the experiment.\n",
    "- `SANDWICH` is a flag for using the {term}`sandwich functor <sandwich functor>`: True to apply the sandwich functor on the circuits, False to apply the usual semantic {term} `functor <functor>`.\n",
    "- `FFL` is a flag for activating the foliated {term}`frames <frame>`. True to set different parameters for the different layers of frames, False to set the same parameter for all the layers. It is to note that it only makes sense to have this flag if the sandwich functor is activated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28993a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we store all the variables needed for the rest of the code: file paths, configurations, parameters...\n",
    "\n",
    "# The path of the file where the initial babI6 data is stored\n",
    "FILEPATH = '../examples/datasets/babi6_10k.txt'\n",
    "\n",
    "# Maximum length of the context\n",
    "TEXT_LENGTH = 4\n",
    "\n",
    "# Maximum Number of wires in a circuit\n",
    "MAX_WIDTH = 9\n",
    "\n",
    "# SANDWICH functor flag\n",
    "SANDWICH = True\n",
    "\n",
    "# Updating the FFL parameter\n",
    "FFL = False\n",
    "\n",
    "# Paths of resulting files for the datasets\n",
    "TRAINING_DATASET_FILEPATH = 'circuits/tutorial_training_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "VALIDATION_DATASET_FILEPATH = 'circuits/tutorial_validation_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "TEST_DATASET_FILEPATH = 'circuits/tutorial_test_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b2f18",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing Function\n",
    "\n",
    "The next step is to write a function `task_file_reader`, which processes the bAbI6 dataset and returns a list of texts, a list of questions on these texts, a list of answers to these questions, and a list of the lengths of the texts. This function reads and cleans lines from the `FILEPATH`, splits lines into stories, and extracts text sentences, questions, and answers.\n",
    "\n",
    "After extracting the texts, they are filtered to only keep the ones whose number of sentences is less than or equal to `TEXT_LENGTH`, which we set in the previous cell to determine the maximum number of sentences that we want in a text for better efficiency. This is to make sure that we do not get huge circuits later on when we convert the texts into circuits, which might slow down the experiment. \n",
    "\n",
    "After this filtering, the last step is to convert the list of texts from a list of arrays of sentences, to a list of sentences. In other words, we concatenate the sentences in each text (which is an array) to obtain a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05e8f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the texts, questions, expected answers, and text_length from the TXT file\n",
    "def task_file_reader(path : str | Path) -> Tuple[List[List[str]],\n",
    "                                                       List[str],\n",
    "                                                       List[str],\n",
    "                                                       List[str]]:\n",
    "    \"\"\"\n",
    "    reads the .txt file at `path`\n",
    "    returns 3 lists of equal length\n",
    "    - text sentences, questions, answers and text length\n",
    "    \"\"\"\n",
    "    with open(path) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # split the lines into stories\n",
    "    # record the first line location of new stories\n",
    "    story_splits = [i for i, line in enumerate(lines) if line[0:2] == '1 ']\n",
    "    # have no more need for line indices - delete these\n",
    "    lines = [' '.join(line.split(' ')[1:]) for line in lines]\n",
    "    # also delete . and \\n\n",
    "    lines = [line.replace('.', '').replace('\\n','') for line in lines]\n",
    "    stories = [lines[i:j] for i, j in zip(story_splits, story_splits[1:]+[None])]\n",
    "\n",
    "    # create text and QnA pairs\n",
    "    texts = []\n",
    "    qnas = []\n",
    "    text_length = []\n",
    "    for story in stories:\n",
    "        # record the lines in the story corresponding to questions\n",
    "        question_splits = [i for i, line in enumerate(story) if '?' in line]\n",
    "        for index in question_splits:\n",
    "            # record the text corresponding to each question\n",
    "            ctx = [line.lower() for line in story[:index] if '?' not in line]\n",
    "            texts.append(ctx)\n",
    "            text_length.append(len(ctx))\n",
    "            # record the question\n",
    "            qnas.append(story[index])\n",
    "\n",
    "    # split qna into questions and answers\n",
    "    questions = [qna.split('\\t')[0].lower().rstrip()[:-1] + \" ?\" for qna in qnas]\n",
    "    answers = [qna.split('\\t')[1].lower() for qna in qnas]\n",
    "\n",
    "    # Filtering the data   \n",
    "    filtered_data = [\n",
    "    (text, question, answer, text_length)\n",
    "    for text, question, answer, text_length in zip(texts, questions, answers, text_length)\n",
    "    if len(text) <= TEXT_LENGTH\n",
    "    ]\n",
    "\n",
    "    # Applying the filter\n",
    "    texts, questions, answers, text_length = map(list, zip(*filtered_data))\n",
    "\n",
    "    # Converting the texts from arrays of sentences to strings\n",
    "    processed_texts_list = []\n",
    "    for text in texts:\n",
    "        processed_text = \"\"\n",
    "        for sentence in text:\n",
    "            processed_text += sentence + \". \"\n",
    "                \n",
    "        processed_texts_list.append(processed_text)\n",
    "    \n",
    "\n",
    "    return processed_texts_list, questions, answers, text_length\n",
    "\n",
    "\n",
    "texts, questions, answers, text_lengths = task_file_reader(FILEPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67428372",
   "metadata": {},
   "source": [
    "## 3. Converting The Texts into Circuits\n",
    "\n",
    "Now that we have our texts and the rest of data ready and pre-processed, we move on to the crucial step of converting them into circuits. We first start by initializing the {py:class}`~lambeq.experimental.discocirc.DisCoCircReader`, then we use the {py:meth}`~lambeq.experimental.discocirc.DisCoCircReader.text2circuit` with the `SANDWICH` argument indicating whether to use the {term}`sandwich functor <sandwich functor>` or not, as well as the `FFL` argument which indicates whether to assign different parameters to the different layers of the frame, or the same parameters.\n",
    "\n",
    "Moreover, we store the data in a dictionary where each entry includes the text, the corresponding generated {term}`DisCoCirc` circuit, the question, the answer, and the text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbca676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# making the circuits from the texts and storing them in the dictionary\n",
    "reader = DisCoCircReader()\n",
    "datadict = {}\n",
    "for i, (text, quest, ans, text_length) in enumerate(zip(texts, questions, answers, text_lengths)):\n",
    "    datadict.update({i:{'text':text, 'dsc_diag': reader.text2circuit(text, sandwich=SANDWICH, foliated_frame_labels = FFL),  'question':quest, 'answer':ans, 'text_length': text_length}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150eddc",
   "metadata": {},
   "source": [
    "## 4. Converting The Circuits from DisCoCirc Circuits to Quantum Circuits\n",
    "While we have the circuits corresponding to the texts ready, they are still {term}`DisCoCirc` circuits, not quantum circuits. Therefore, we need to convert the DisCocirc circuits into {term}`quantum circuits <quantum circuit>` by applying an {term}`ansatz <ansatz (plural: ansätze)>`. In this case, we chose to apply the Sim4Ansatz with 3 layers, and one {term}`qubit <qubit>` for each noun. More information on the motivation behind this choice can be found [here](https://arxiv.org/pdf/2409.08777)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73465ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq import Sim4Ansatz\n",
    "from lambeq import AtomicType\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "\n",
    "for i in datadict.keys():\n",
    "    datadict[i].update({'text_circuit_sim4_13': ansatz(datadict[i]['dsc_diag'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdeb9a1",
   "metadata": {},
   "source": [
    "## 5. Assertion Circuits and Further Processing of the Circuits\n",
    "The main spirit of this tutorial is having assertion circuits sequetially composed with the text circuits to see the similarity between the texts and the assertions. More details on assertions and question asking in general can be found [here](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "Now that we already have the circuits representing the texts, we need to make the circuits representing the assertions. Remember, in our experiment, we need to have a pair of circuits, one for the affirmative case, and the other for the negative case. However, when adding the box corresponding to the assertion, we have to make sure that the wires of the assertion box correspond to the nouns from the text.  \n",
    "\n",
    "Below, the function `return_noun_list` returns all the nouns in a text. The function `return_q_nouns` return all the nouns in a question. In the latter, we take the second and fifth noun as the subject and object of the question respectively. This works because of the simple case of the bAbI6 experiments, all the questions are of the format \"Is the subject in the location?\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15baa8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Ty\n",
    "\n",
    "def return_noun_list(text):\n",
    "    noun_list = []\n",
    "    for b in text.boxes:\n",
    "        if b.dom == Ty() and b.cod == N:\n",
    "            noun_list.append(b.name)\n",
    "            \n",
    "    return noun_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32d767fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_q_nouns(question):\n",
    "    question_words = question.split(' ')\n",
    "    q_nouns = [question_words[1], question_words[4].strip('?')]\n",
    "    return q_nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae48f77",
   "metadata": {},
   "source": [
    "We proceed to add the lists obtained from the functions above to the dictionary to be used later when building the assertion boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeefa4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in datadict.keys():\n",
    "    datadict[i].update({'noun_list_text': return_noun_list(datadict[i]['dsc_diag'])})\n",
    "    datadict[i].update({'noun_list_question': return_q_nouns(datadict[i]['question'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a2617",
   "metadata": {},
   "source": [
    "We needed to extract the list of nouns in the texts and the list of nouns in their corresponding questions to remove the entries where we ask questions on subjects or locations not present in the text. We also filter by text length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b15b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_datadict = {\n",
    "    i: entry \n",
    "    for i, entry in datadict.items() \n",
    "    if set(entry['noun_list_question']).issubset(entry['noun_list_text']) and entry['text_length'] < TEXT_LENGTH\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fb6e7e",
   "metadata": {},
   "source": [
    "Moreover, remember that to enhance performance, we also wanted to limit the number of wires in every circuit by chekcing that every circuit's codomain (which is the number of open wires of a circuit) is less than or equal to `MAX_WIDTH`. The following filters the entries in the `datadict` dictionary and only keep the entries in which the text circuits have less than or equal to  the maximum number of wires specified in `MAX_WIDTH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af318e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reducing the size of the dictionary by removing the circuits that have more than a certain number of wires. \n",
    "def right_cod_size(circuit):\n",
    "    if len(circuit.cod) > MAX_WIDTH:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered_cod_datadict = {\n",
    "    i: entry \n",
    "    for i, entry in reduced_datadict.items() \n",
    "    if right_cod_size(entry['text_circuit_sim4_13'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04a8ff9",
   "metadata": {},
   "source": [
    "Now that the text circuits are post-processed for optimization, it is time to make the assertion circuits to later sequentially compose the latter with the former. \n",
    "\n",
    "We first start with constructing two boxes `q1` and `q2` for both the affirmative and negative assertions respectively. An affirmative assertion corresponds to an affirmative answer to the question. On the other hand, a negative assertion corresponds to a negative answer to the question. For example, if the question related to a text is \"Is Emily in the kitchen?\", the equivalent negative assertion would be \"Emily not in the kitchen\". For the purposes of this training, all the questions are either in the format of \"Is subject in object?\" or \"Is subject not in object\". Therefore, we will need two boxes for the assertions, a box for the \"is in\" assertions, and another for the \"is not in\" assertions. The purpose of having two generic boxes is that the ML model will learn later the parameters for these boxes. For more details about question asking in {term}`DisCoCirc`, we recommend [this paper](https://arxiv.org/pdf/2409.08777).\n",
    "\n",
    "Notice that we also created two assertion boxes that are equiped with swaps, the purpose of which will become clearer in later parts of the tutorial. \n",
    "\n",
    "We apply the same {term}`ansatz <ansatz (plural: ansätze)>` applied on the text circuits (Sim4Ansatz with 3 layers and one qubit for each wire). Lastly, we add the postselections by sequentially composing the resulting circuit from applying the {term}`ansatz <ansatz (plural: ansätze)>` to a parallel composition of two effects (bras). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ea6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lambeq.backend.grammar import Box, Id\n",
    "from lambeq.backend.quantum import  Bra, Discard, qubit, Id, Swap\n",
    "from lambeq import AtomicType, Sim4Ansatz\n",
    "\n",
    "N = AtomicType.NOUN\n",
    "\n",
    "q1 = Box('is_in', N@N, N@N)\n",
    "q2 = Box('is_not_in', N@N, N@N)\n",
    "\n",
    "ansatz = Sim4Ansatz({N:1}, n_layers=3)\n",
    "qcirc1 = ansatz(q1)\n",
    "qcirc2 = ansatz(q2)\n",
    "\n",
    "#add the postselections to the questions\n",
    "qcirc1_final = qcirc1 >> Bra(0) @ Bra(0)\n",
    "qcirc2_final = qcirc2 >> Bra(0) @ Bra(0)\n",
    "\n",
    "is_in_q = qcirc1_final\n",
    "is_not_in_q = qcirc2_final\n",
    "\n",
    "is_in_q_swp = Swap(qubit, qubit) >> qcirc1 >> Bra(0) @ Bra(0)\n",
    "is_not_in_q_swp = Swap(qubit, qubit) >> qcirc2 >> Bra(0) @ Bra(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f120a",
   "metadata": {},
   "source": [
    "## 5. Assembling The Text Circuits with the Question Circuits\n",
    "\n",
    "Now that we have all the ingredients in place (the text and assertion circuits), it is time to assemble them using sequential composition. However, we need to be careful and only attach the wires in the question boxes to the corresponding wires in the text boxes so that the nouns match. Moreover, we have to discard the wires of the nouns that are not included in the question. In order to do this, we might need to perform some swaps so that the wires that get composed with the question circuit are the corresponding wires from the text circuit.\n",
    "\n",
    "We start by creating a layer composed of either identities (to link with the wires corresponding to the question nouns), or discards (for the rest of the wires). Once we sequentially compose this layer with the text circuit, this leaves us with a circuit whose codomain has two wires corresponding to the question nouns. In order for us to attach the assertion boxes, we have to make sure that the wires from the assertion circuits are linked to the right wires from the text circuit. To achieve this, we check the question ids of the wires in the text circuits (to see whether the nouns in the text circuits are in the right order). This helps us decide whether to use the assertion boxes that come with swaps, or the ones without swaps (if the question wires are in the wrong order, we would need a swap to bring them back to the right order for the questions. Remember, we already created assertion boxes that are also equiped with swaps for this purpose).\n",
    "\n",
    "Notice that, throughout the next cell, we always have two circuits. The circuit names ending in \"pos\" signal the circuits corrsponding to the affirmative assertions, while their counterparts ending in \"neg\" signal the ones corresponding to the negative assertions.\n",
    "\n",
    "**Important note**: It is to note that this isn't the standard way to present the questions/assertions (in this approach, we opted for using post-selections directly), and we went for this approach for the sake of simplicity for this tutorial. More complex approaches to assertions on text can be found [here](https://arxiv.org/pdf/2409.08777). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f50a873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing everything together by plugging the question asking part to the text circuits\n",
    "for i in reduced_datadict.keys():\n",
    "    \n",
    "    text_circuit = reduced_datadict[i]['text_circuit_sim4_13']\n",
    "    text_nouns = datadict[i]['noun_list_text']\n",
    "    q_nouns = datadict[i]['noun_list_question']\n",
    "    qid1 = datadict[i]['noun_list_text'].index(q_nouns[0])\n",
    "    qid2 = datadict[i]['noun_list_text'].index(q_nouns[1])\n",
    "\n",
    "    swap_required = qid1 > qid2\n",
    "    \n",
    "    if qid1 == qid2:\n",
    "        print('noun ids are idential, removing entry')\n",
    "        del reduced_datadict[i]\n",
    "        continue\n",
    "\n",
    "    quest_mid_layer = Id(qubit) if (qid1 == 0 or qid2 == 0) else Discard()\n",
    "    \n",
    "    for k in range(1, len(text_circuit.cod)):\n",
    "        if k == qid1 or k == qid2:\n",
    "            quest_mid_layer = quest_mid_layer @ Id(qubit)\n",
    "        else:\n",
    "            quest_mid_layer = quest_mid_layer @ Discard()\n",
    "\n",
    "    final_circuit = text_circuit >> quest_mid_layer\n",
    "\n",
    "    if swap_required:\n",
    "        final_circuit_pos = final_circuit >> is_in_q_swp\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q_swp\n",
    "    else:\n",
    "        final_circuit_pos = final_circuit >> is_in_q\n",
    "        final_circuit_neg = final_circuit >> is_not_in_q\n",
    "\n",
    "    reduced_datadict[i].update({'pos_neg_circuit_pair': (final_circuit_pos, final_circuit_neg)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd17926",
   "metadata": {},
   "source": [
    "## 6. Preparing The Datasets for Training\n",
    "Now that our circuit pairs are ready, we move on to the final step of training a model.\n",
    "\n",
    "The first step is to prepare the data for training. We start with updating the \"yes\" and \"no\" entries to 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc8aecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bAbI6_datadict = {}\n",
    "for i in reduced_datadict.keys():\n",
    "    # Add the updated dictionary with the transformed 'answer'\n",
    "    bAbI6_datadict.update({\n",
    "        i: {\n",
    "            'text': reduced_datadict[i]['text'],\n",
    "            'question': reduced_datadict[i]['question'],\n",
    "            'answer': 1 if reduced_datadict[i]['answer'] == 'yes' else 0,  # Transform 'yes' to 1 and 'no' to 0\n",
    "            'quantum_circ_pair_pos_neg': reduced_datadict[i]['pos_neg_circuit_pair'],\n",
    "            'text_length': reduced_datadict[i]['text_length']\n",
    "        }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3545dc09",
   "metadata": {},
   "source": [
    "The next step would be to make three sets: training, validation, and test sets. We try to balance the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcecde44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the minimum size is: 89\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add the 'measure' field to each item\n",
    "for key, value in bAbI6_datadict.items():\n",
    "    temp = -1 if value['answer'] == 0 else 1\n",
    "    value['measure'] = temp * value['text_length']\n",
    "\n",
    "# Group items by absolute value of measure\n",
    "abs_value_groups = defaultdict(list)\n",
    "for key, value in bAbI6_datadict.items():\n",
    "    abs_value = abs(value['measure'])\n",
    "    abs_value_groups[abs_value].append((key, value))\n",
    "\n",
    "# Balance signs within each group and ensure diverse sizes\n",
    "new_balanced_dict = {}\n",
    "for abs_value, items in abs_value_groups.items():\n",
    "    # Separate positive and negative items\n",
    "    positive_items = [(k, v) for k, v in items if v['measure'] > 0]\n",
    "    negative_items = [(k, v) for k, v in items if v['measure'] < 0]\n",
    "    \n",
    "    # Determine the maximum balanced size for this group\n",
    "    min_size = min(len(positive_items), len(negative_items))\n",
    "    print(\"the minimum size is: \" + str(min_size))\n",
    "    \n",
    "    # Randomly sample from each group to balance\n",
    "    balanced_positive = random.sample(positive_items, min_size)\n",
    "    balanced_negative = random.sample(negative_items, min_size)\n",
    "    \n",
    "    # Add to the balanced dictionary\n",
    "    for k, v in balanced_positive + balanced_negative:\n",
    "        new_balanced_dict[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b7c34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Convert dictionary into list of keys and values\n",
    "keys = list(new_balanced_dict.keys())\n",
    "values = list(new_balanced_dict.values())\n",
    "\n",
    "# Split into training and temporary (validation + testing)\n",
    "train_keys, temp_keys, train_values, temp_values = train_test_split(\n",
    "    keys, values, test_size=0.4, random_state=42  # 60% training, 40% temp\n",
    ")\n",
    "\n",
    "# =Split the temporary set into validation and testing\n",
    "val_keys, test_keys, val_values, test_values = train_test_split(\n",
    "    temp_keys, temp_values, test_size=0.5, random_state=42  # 20% validation, 20% testing\n",
    ")\n",
    "\n",
    "# Reconstruct dictionaries for training, validation, and testing\n",
    "training_dict_bAbI6 = {k: v for k, v in zip(train_keys, train_values)}\n",
    "validation_dict_bAbI6 = {k: v for k, v in zip(val_keys, val_values)}\n",
    "test_dict_bAbI6 = {k: v for k, v in zip(test_keys, test_values)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37e9614",
   "metadata": {},
   "source": [
    "The following cell is to check that we have a balanced set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd61567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "yes_count = 0\n",
    "no_count = 0\n",
    "for i in training_dict_bAbI6:\n",
    "    if training_dict_bAbI6[i]['answer'] == 0:\n",
    "        no_count += 1\n",
    "    else:\n",
    "        yes_count += 1\n",
    "\n",
    "print(yes_count)\n",
    "print(no_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af769549",
   "metadata": {},
   "source": [
    "Now, the final step is to store all of this data in separate files for training, validation, and testing, to be used in part II of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5a05cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths of resulting files for the datasets\n",
    "TRAINING_DATASET_FILEPATH = 'tutorial_training_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "VALIDATION_DATASET_FILEPATH = 'tutorial_validation_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'\n",
    "TEST_DATASET_FILEPATH = 'tutorial_test_ffl' + str(FFL) + '_sandwich_' + str(SANDWICH) + '.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f1a78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TRAINING_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(training_dict_bAbI6, file)\n",
    "with open(VALIDATION_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(validation_dict_bAbI6, file)\n",
    "with open(TEST_DATASET_FILEPATH, 'wb') as file:\n",
    "    pickle.dump(test_dict_bAbI6, file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tutorials_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
